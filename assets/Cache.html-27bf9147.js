import{_ as t,V as o,W as p,X as s,Z as l,a0 as i,Y as n,$ as e,F as c}from"./framework-c6791857.js";const d={},r=e('<h1 id="缓存" tabindex="-1"><a class="header-anchor" href="#缓存" aria-hidden="true">#</a> 缓存</h1><h2 id="为什么要用缓存" tabindex="-1"><a class="header-anchor" href="#为什么要用缓存" aria-hidden="true">#</a> 为什么要用缓存?</h2><blockquote><p>项目里怎么用的? 为啥用? 没用好会有啥后果?</p></blockquote><p><strong>用缓存的理由:</strong></p><ul><li>高性能: 在内存中, 只有简单的 k-v 对, 性能高.</li><li>高并发: MySQL 较重, 并发支持不好, 使用缓存简化操作, 提高并发量.</li></ul><p><strong>如何使用的:</strong></p><ul><li>结合项目进行阐述, 比如将热点商品放入缓存, 验证码放入缓存等等.</li></ul><h2 id="缓存问题-数据一致性" tabindex="-1"><a class="header-anchor" href="#缓存问题-数据一致性" aria-hidden="true">#</a> 缓存问题: 数据一致性</h2><blockquote><p>主要是缓存和数据库双存储双写, 会导致数据一致性问题.</p></blockquote><p><strong>严格缓存/数据库一致性:</strong></p><ul><li>读写请求串行化, 都放到同一个内存队列.</li><li>效率很低</li></ul><p><strong>Cache Aside Pattern:</strong></p><ul><li>读操作: 读缓存, 缓存没有就去数据库读, 然后取出数据放入缓存.</li><li>写操作: 先写数据库, 然后删除缓存. <ul><li>为什么直接删除缓存: 更新缓存可能涉及很复杂的计算路基, 而且不能保证该缓存会被频繁访问. 用到缓存再去算缓存, 这样开销更少.</li></ul></li></ul><h3 id="cache-aside-pattern" tabindex="-1"><a class="header-anchor" href="#cache-aside-pattern" aria-hidden="true">#</a> Cache Aside Pattern</h3><p><strong>先写数据库, 再删除缓存:</strong></p><ul><li>如果缓存删除失败, 那么数据库中是新数据, 缓存中是旧数据, 数据不一致.</li><li>先删缓存, 再写数据库. 先删缓存不会导致数据不一致, 只是数据库写失败之后是旧数据(可以重新请求操作).</li><li>延迟双删: 先写数据库, 再删缓存, delay 一段时间再删除一次缓存.</li></ul><p><strong>高并发读写数据, 删除缓存后到数据库找, 但是上一次的数据库写操作还没有执行完, 读取到了旧数据, 更新到缓, 数据库写操作结束后, 缓存和数据库消息不一致.</strong></p><ul><li>写数据库时, 根据数据唯一 id, 将对相同数据的操作路由到同一个同步队列, 对应一个工作线程, 串行执行应操作. 如果数据不在缓存中, 那么把&quot;读取数据 + 更新缓存&quot;的操作以同样的方式放入另一个同步队列.</li><li>优化: 避免相连的更新缓存请求进入队列, 直接让后续的更新请求自旋等待前面的更新操作完成.</li><li>排队带来的阻塞问题: 读请求排队可能会使请求阻塞较长时间, 可以引入超时机制, 如果排队超时, 就直接去旧数据. 但是过多读请求排队超时会让请求直接走数据库, 致使缓存击穿. <ul><li>解决: 加机器, 注意热点商品路由, 避免路由到同一台机器的同一个队列.</li></ul></li></ul><h2 id="缓存问题-缓存雪崩" tabindex="-1"><a class="header-anchor" href="#缓存问题-缓存雪崩" aria-hidden="true">#</a> 缓存问题: 缓存雪崩</h2><blockquote><p>缓存必问问题, 出现之后很致命.</p><p>出现情况: 缓存最多抗住 4000 QPS, 高峰请求 5000 QPS, 但是缓存挂了. 5000 请求全部落到数据库, 数据库扛不住, 挂掉, 重启后还是扛不住, 继续挂.</p></blockquote><p><strong>解决方案:</strong></p><ul><li>事前: Redis 高可用, 主从 + 哨兵, Redis Cluster, 避免全盘崩溃.</li><li>事中: 本地 EhCache 缓存 + hystrix 限流/降级, 避免 MySQL 扛不住.</li><li>事后: Redis 持久化, Redis 重启后, 自动从磁盘加载数据, 快速恢复缓存数据.</li></ul><blockquote><p>限流: 限制单位时间内能够通过的请求数.</p><p>降级: 不真正处理请求, 而是返回一些默认值或者提示.</p></blockquote><p>这样做的优点是 MySQL 一定不会挂掉, 至少能满足一部分用户的请求.</p><h2 id="缓存问题-缓存穿透" tabindex="-1"><a class="header-anchor" href="#缓存问题-缓存穿透" aria-hidden="true">#</a> 缓存问题: 缓存穿透</h2><blockquote><p>出现情况: 5000 QPS, 但是 4000 QPS 是恶意攻击, 在缓存中查不到(比如 id 为负数). 每次请求都直接走数据库, 不经过缓存, 就像&quot;穿透了缓存&quot;一样. 然后数据库就 G 啦.</p></blockquote><p><strong>解决方案:</strong></p>',27),u=s("li",null,[n("缓存写空值: "),s("ul",null,[s("li",null,"每次只要从数据没有查到数据, 就向缓存中对应的 key 写一个空值, 然后设置过期时间."),s("li",null,"问题: 如果每次攻击使用不同的 id, 写空值就无效了.")])],-1),h=s("ul",null,[s("li",null,"在缓存之前添加布隆过滤器, 将数据库所有可能的数据 hash 映射到布隆过滤器中."),s("li",null,[n("对每个请求进行如下判断: "),s("ul",null,[s("li",null,"如果请求数据的 key 不在布隆过滤器中, 那么也一定不在数据库中, 直接返回不存在."),s("li",null,"如果请求数据的 key 在布隆过滤器中, 就执行正常查询流程.")])])],-1),k=e(`<h2 id="缓存问题-缓存击穿" tabindex="-1"><a class="header-anchor" href="#缓存问题-缓存击穿" aria-hidden="true">#</a> 缓存问题: 缓存击穿</h2><blockquote><p>针对热点 key 的问题, 如果在集中式高并发场景下, <strong>当 key 失效</strong>, 大量请求击穿缓存, 直接请求数据库, 数据库也就 G 啦.</p></blockquote><p><strong>按照场景确定解决方式:</strong></p><ul><li>如果缓存的数据几乎不会发生更新, 那么直接将热点数据设置为永不过期.</li><li>如果缓存的数据更新不频繁, 并且刷新缓存耗时少, 可以使用分布式中间件的分布式互斥锁/本地锁来保证只有少量请求可以请求数据库, 并且刷新缓存, 其余线程等待锁释放后可以访问新缓存.</li><li>如果缓存的数据更新频繁或者刷新缓存耗时长, 可以利用定时线程在缓存过期前主动重新构建缓存或者延长缓存过期时间, 保证所有请求能够访问到对应的缓存.</li><li>避免同时很多 key 失效, 将 key 的过期时间加上一个随机数.</li></ul><h2 id="缓存问题-大-key-问题" tabindex="-1"><a class="header-anchor" href="#缓存问题-大-key-问题" aria-hidden="true">#</a> 缓存问题: 大 key 问题</h2><blockquote><p>大 key: 一个 Redis key 中存储了过大或者过多的数据. 如: MB 级别的 String, W 级别的数据量.</p></blockquote><p><strong>导致的问题:</strong></p><ul><li>Client 执行命令速度变慢.</li><li>内存溢出或者重要的 key 被清除.</li><li>集群下数据分片使用率不均等, 难以负载均衡.</li><li>处理大 key 时, Redis 自身服务变慢, 波及相关服务.</li><li>处理大 key 时, 主库可能长时间阻塞, 导致同步中断甚至主从切换.</li></ul><p><strong>解决方案:</strong></p><ul><li>拆分大 key.</li><li>清理大 key, 使用其他存储方式进行存储.</li><li>定期清理过期的数据.</li><li>监控 Redis 内存水位.</li></ul><h2 id="缓存问题-并发竞争" tabindex="-1"><a class="header-anchor" href="#缓存问题-并发竞争" aria-hidden="true">#</a> 缓存问题: 并发竞争</h2><blockquote><p>多客户端同时并发写一个 key, 导致数据错误.</p><p>等同于问: Redis 事务的 CAS　方案.</p></blockquote><p><strong>解决方案:</strong></p><ul><li>基于 Zookeeper 实现分布式锁, 每个系统都通过 Zookeeper 获取分布式锁, 保证同一时间只有一个系统实例在操作某个 key.</li><li>读入缓存时加入版本号或者时间戳, 比较版本号/时间戳是否比缓存中的更新, 避免用旧的数据覆盖新数据.</li></ul><h2 id="redis-线程模型" tabindex="-1"><a class="header-anchor" href="#redis-线程模型" aria-hidden="true">#</a> Redis 线程模型</h2><blockquote><p>Redis 线程模型是什么? 为什么 Redis 单线程却能够支持高并发?</p></blockquote><p>Redis 是单线程工作模型. 只使用单核进行处理.</p><p><strong>为什么是单核的?</strong></p><ul><li>Redis 内部使用文件事件处理器(FileEventHandler), 这个文件事件处理器是单线程的.</li></ul><p><strong>为什么支持多个 Socket?</strong></p><ul><li>采用 I/O 多路复用机制, 同时监听多个 Socket, 将产生事件的 Socket 压入内存队列, 经由事件分派器进行处理.</li><li>事件分派器每次从队列中取出一个 Socket 进行处理.</li></ul><p><strong>文件事件处理器结构?</strong></p><ul><li>多个 Socket</li><li>I/O 多路复用程序</li><li>文件事件分派器</li><li>事件处理器 <ul><li>连接应答处理器(产生 AE_READABLE 事件)</li><li>命令请求处理器(处理 AE_READABLE 事件)</li><li>命令回复处理器(处理 AE_WRITABLE)</li></ul></li></ul><p><strong>为什么 Redis 单线程效率也那么高?</strong></p><ul><li>纯内存操作</li><li>非阻塞的 I/O 多路复用机制</li><li>C 语言实现</li><li>避免多线程上下文切换问题, 预防线程间的竞争</li></ul><p><strong>为什么 Redis 6.0 引入了多线程?</strong></p><ul><li>只是在处理网络数据的读写和协议解析时使用多线程</li><li>执行命令任然是单线程</li><li>性能瓶颈在网络 I/O 上, 多线程减少对 Redis 主线程的阻塞时间</li></ul><h2 id="redis-数据结构" tabindex="-1"><a class="header-anchor" href="#redis-数据结构" aria-hidden="true">#</a> Redis 数据结构</h2><blockquote><p>主要是对各种 Redis 功能的了解.</p></blockquote><p>// 用 go 来描述, String 类型的 RedisObject</p><div class="language-go line-numbers-mode" data-ext="go"><pre class="language-go"><code><span class="token comment">// redis object definition</span>
<span class="token keyword">type</span> RedisObject <span class="token keyword">struct</span> <span class="token punctuation">{</span>
  Type      <span class="token builtin">string</span>
  Encoding  EncodingType
  Ptr       <span class="token operator">*</span>SDS
  <span class="token comment">//...</span>
<span class="token punctuation">}</span>

<span class="token comment">// encoding</span>
<span class="token keyword">type</span> <span class="token punctuation">(</span>
  EncodingType <span class="token builtin">string</span>
  StringEncodingType EncodingType
<span class="token punctuation">)</span>

<span class="token keyword">const</span> <span class="token punctuation">(</span>
  <span class="token comment">// 保存 32 bit 以下的整形</span>
  INT StringEncodingType    <span class="token operator">=</span> <span class="token string">&quot;int&quot;</span>
  <span class="token comment">// 保存大于 32 bit 的字符串</span>
  RAW StringEncodingType    <span class="token operator">=</span> <span class="token string">&quot;raw&quot;</span>
  <span class="token comment">// 保存短字符串(不大于 32 bit), 浮点数</span>
  EMBSTR StringEncodingType <span class="token operator">=</span> <span class="token string">&quot;embstr&quot;</span>
<span class="token punctuation">)</span>

<span class="token comment">// SDB</span>
<span class="token keyword">type</span> SimpleDynamicString <span class="token keyword">struct</span> <span class="token punctuation">{</span>
  <span class="token comment">// buf 剩余的未使用空间</span>
  free  <span class="token builtin">int</span>
  <span class="token comment">// buf 已使用空间, 当前字符串的长度</span>
  <span class="token builtin">len</span>   <span class="token builtin">int</span>
  <span class="token comment">// 字符串缓冲区, 减少字符串长度变化时, 内存分配次数</span>
  buf   <span class="token punctuation">[</span><span class="token punctuation">]</span>char
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>数据类型:</strong></p><ul><li>String(Simple Dynamic String, SDS)</li><li>Hash</li><li>List</li><li>Set</li><li>Sorted Set</li></ul><p><strong>String:</strong></p><ul><li>简单 k-v get/set</li><li>SDS <code>{len, free, buf[]}</code></li><li>encoding: int/raw/embstr</li></ul><p><strong>Hash:</strong></p><ul><li>底层: <ul><li>ziplist: <ul><li>所有 k-v 字符串长度小于 64 字节</li><li>k-v 对数量小于 512 个</li></ul></li><li>hashtable</li></ul></li></ul><p><strong>List:</strong></p><ul><li>有序列表</li><li>底层: <ul><li>ziplist: <ul><li>所有 k-v 字符串长度小于 64 字节</li><li>k-v 对数量小于 512 个</li></ul></li><li>linkedlist</li></ul></li></ul><p><strong>Set:</strong></p><ul><li>无须集合, 自动去重</li><li>底层 <ul><li>intset <ul><li>所有元素都是整数值</li><li>元素数量小于 512 个</li></ul></li><li>hashtable</li></ul></li></ul><p><strong>Sorted Set:</strong></p><ul><li>有序集合, 自动去重</li><li>底层: <ul><li>ziplist <ul><li>元素数量小于 128 个</li><li>所有元素长度小于 64 字节</li></ul></li><li>skiplist</li></ul></li></ul><h2 id="为什么用-skiplist-而不用-红黑树-平衡树" tabindex="-1"><a class="header-anchor" href="#为什么用-skiplist-而不用-红黑树-平衡树" aria-hidden="true">#</a> 为什么用 Skiplist 而不用 红黑树/平衡树?</h2>`,44),m=e(`<h2 id="redis-数据结构在业务中的使用" tabindex="-1"><a class="header-anchor" href="#redis-数据结构在业务中的使用" aria-hidden="true">#</a> Redis 数据结构在业务中的使用</h2><ul><li>List: 关注列表, 最新消息排行, 消息队列...</li><li>Set: 集合相关操作, 交集/并集/差集...</li><li>Zset: 集合排序/延时队列/Top K/范围查找...</li><li>String: 计数器/分布式锁(setnx)/分布式全局 id/验证码...</li><li>Hash: 购物车/商品信息...</li></ul><blockquote><p>商品信息: <code>hmset COMMODITY:BIZ_TYPE:ID field value...</code></p></blockquote><h2 id="redis-过期策略" tabindex="-1"><a class="header-anchor" href="#redis-过期策略" aria-hidden="true">#</a> Redis 过期策略</h2><blockquote><p>有哪些过期策略? 有哪些内存淘汰机制? LRU 代码实现?</p></blockquote><p><strong>设置过期方法?</strong></p><p>对于 hash 来说, <strong>hash 的过期时间既可以作用在 field 上, 也可以作用在 key 上</strong>. 使用如下指令:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 设置 key 在 timestamp s 之后过期, 单位为 s</span>
EXPIRE key timestamp
<span class="token comment"># 设置 key 在 timestamp ms 之后过期, 单位为 ms</span>
PEXPIRE key timestamp

<span class="token comment"># 设置 hashKey 某个 field 在 timestamp s 之后过期, 单位为 s</span>
HEXPIRE hashKey timestamp
<span class="token comment"># 设置 hashKey 某个 field 在 timestamp ms 之后过期, 单位为 ms</span>
HPEXPIRE hashKey timestamp

<span class="token comment"># 设置 key 在 timestamp s 时过期, 单位为 s</span>
EXPIREAT key timestamp
<span class="token comment"># 设置 key 在 timestamp ms 时过期, 单位为 ms</span>
PEXPIREAT key timestamp

<span class="token comment"># 移除 key 过期时间</span>
PERSIST key
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>过期策略?</strong></p><ul><li>定期删除 + 惰性删除</li><li>定期删除: <ul><li>每隔一定时间就从 expires 字典中检查一些(随机抽取)设置了过期时间的 key, 如果过期了, 就将其删除.</li></ul></li><li>惰性删除: <ul><li>获取 key 时, 检查 key 是否设置了过期时间并且过期了? 如果过期就删除.</li></ul></li><li>大量过期 key 堆积问题: <ul><li>使用内存淘汰机制.</li></ul></li></ul><p><strong>内存淘汰机制?</strong></p><ul><li>noeviction: 当内存不足以容纳新写入数据时, 新写入操作会报错.</li><li>allkeys-lru: 当内存不足以容纳新写入数据时, 在键空间中, 移除最近最少使用的 key(这个是最常用的)</li><li>allkeys-random: 当内存不足以容纳新写入数据时, 在键空间中, 随机移除某个 key.</li><li>volatile-lru: 当内存不足以容纳新写入数据时, 在设置了过期时间的键空间中, 移除最近最少使用的 key.</li><li>volatile-random: 当内存不足以容纳新写入数据时, 在设置了过期时间的键空间中, 随机移除某个 key.</li><li>volatile-ttl: 当内存不足以容纳新写入数据时, 在设置了过期时间的键空间中, 有更早过期时间的 key 优先移除.</li></ul><p><strong>LRU 代码实现?</strong></p><div class="language-java line-numbers-mode" data-ext="java"><pre class="language-java"><code><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">LRUCache</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">K</span><span class="token punctuation">,</span> <span class="token class-name">V</span><span class="token punctuation">&gt;</span></span> <span class="token keyword">extends</span> <span class="token class-name">LinkedHashMap</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">K</span><span class="token punctuation">,</span> <span class="token class-name">V</span><span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span>
  <span class="token keyword">private</span> <span class="token keyword">int</span> capacity<span class="token punctuation">;</span>

  <span class="token keyword">public</span> <span class="token class-name">LRUCache</span><span class="token punctuation">(</span><span class="token keyword">int</span> capacity<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">super</span><span class="token punctuation">(</span>capacity<span class="token punctuation">,</span> <span class="token number">0.75f</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span>
    <span class="token keyword">this</span><span class="token punctuation">.</span>capacity <span class="token operator">=</span> capacity<span class="token punctuation">;</span>
  <span class="token punctuation">}</span>

  <span class="token annotation punctuation">@Override</span>
  <span class="token keyword">protected</span> <span class="token keyword">boolean</span> <span class="token function">removeEldestEntry</span><span class="token punctuation">(</span><span class="token class-name">Map<span class="token punctuation">.</span>Entry</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">K</span><span class="token punctuation">,</span> <span class="token class-name">V</span><span class="token punctuation">&gt;</span></span> eldest<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">return</span> <span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> capacity<span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="redis-高并发和高可用" tabindex="-1"><a class="header-anchor" href="#redis-高并发和高可用" aria-hidden="true">#</a> Redis 高并发和高可用</h2><blockquote><p>Redis 单机能承载多少并发? 如果单机抗不住怎么扩容?</p><p>Redis 会挂掉吗? 怎么保证高可用?</p><p>Redis 实现高并发主要依靠主从架构, 一主多从. 单主用于写数据, 多从用于读数据.</p><p>要高并发 + 容纳大量数据, 就要搭建 Redis 集群.</p></blockquote><p>缓存主要是用来支持读高并发的.</p><p><strong>读写分离:</strong> 主节点负责写操作, 并且将数据复制到其他从节点, 从节点只负责读.</p><p>所有读请求全走从节点, 容易实现水平扩容, 支撑读高并发.</p><blockquote><p>这部分比较难, 等待后期补坑</p></blockquote><h2 id="redis-持久化" tabindex="-1"><a class="header-anchor" href="#redis-持久化" aria-hidden="true">#</a> Redis 持久化</h2><blockquote><p>Redis 的持久化有哪几种方式? 不同的持久化机制都有什么优缺点? 持久化机制具体底层是如何实现的?</p></blockquote><p><strong>为什么要进行持久化?</strong> 如果 Redis 只将数据存储在内存中, 如果 Redis 挂掉, 那么重启后 Redis 中就没有任何数据, 变得不可用了.</p><p>如果将 Redis 数据持久化到磁盘中, 那么即使 Redis 挂掉, 重启 Redis 之后还可以加载持久化的数据, 可以继续使用.</p><p><strong>Redis 持久化的方式?</strong></p><ul><li>RDB 持久化: 对 Redis 中的数据执行周期性的持久化. <code>SAVE</code> 主进程阻塞进行持久化, <code>BGSAVE</code> 新建一个子进程异步持久化.</li><li>AOF 持久化: 将每条写命令作为日志, 以 append-only 的模式写入一个 AOF 文件中. 在 Redis 重启时, 可以回放 AOF 日志中的写入命令来重新构建整个缓存数据库.</li><li>当两种持久化方式都存在时, 会优先使用 AOF 日志.</li></ul><p><strong>AOF 优缺点?</strong></p><ul><li>Redis 可以更完好的保护数据不丢失, 默认 AOF 会每隔 1s, 通过<strong>后台线程</strong>执行一次 fsync 操作, 也就是最多会丢失 1s 的数据.</li><li>AOF 文件是 append-only 的写入方式, 没有磁盘寻址的开销, 写入性能高, 文件不容易破损.</li><li>AOF 日志文件即使过大的时候, 出现后台重写操作, 也不会影响客户端的读写. 重写时有 AOF 写缓冲区, 当新 AOF 创建完成之后, 再将缓冲区中的日志写入.</li><li>AOF 日志文件的命令通过可读较强的方式进行记录, 这个特性非常适合做灾难性的误删除的紧急恢复.</li><li>AOF 日志文件通常比 RDB 数据快照文件更大.</li><li>AOF 开启后, 支持的写 QPS 会比 RDB 支持的写 QPS 低, 因为每秒要执行一次 fsync 操作, 占用部分处理器资源.</li></ul><p><strong>如何选择 AOF 和 RDB?</strong></p><ul><li>不能只使用 RDB, 可能会用较大的数据丢失.</li><li>仅仅使用 AOF 会导致两个问题: <ul><li>AOF 冷备份速度没有 RDB 快.</li><li>RDB 生成的数据库快照更完备和健壮, 可以避免 AOF 复杂备份和恢复机制带来的 BUG.</li></ul></li><li>建议使用 AOF + RDB, AOF 保证数据不丢失, RDB 用作冷备份, 作为 AOF 出问题的后备数据恢复手段.</li></ul><h2 id="redis-rehash" tabindex="-1"><a class="header-anchor" href="#redis-rehash" aria-hidden="true">#</a> Redis Rehash</h2><blockquote><p>Redis 的 hashtable 初始容量有限, 不足时需要扩容, 扩容就需要对数据进行 rehash, 如果一次进行全部 rehash 操作, 肯定会阻塞整个 Redis.</p><p>那就用, 渐进式 Rehash !</p></blockquote><p><strong>备用哈希表:</strong></p><ul><li>Redis 的 hashtable 中有两个哈希表, index 分别为 0 和 1, 正常只使用 <code>ht[0]</code>, <code>ht[1]</code> 闲置.</li><li>进行 Rehash 操作时, 启用 <code>ht[1]</code>, 将 <code>ht[0]</code> 中的数据渐进式的 Rehash, 放入 <code>ht[1]</code> 中.</li><li>Rehash 时, 读写据会同时从两张表中读, 写数据只会向 <code>ht[1]</code> 中写, <code>ht[0]</code> 中的数据只减不增, 最后变为空表.</li></ul><blockquote><p>rehash 移动的是 key 指向的 value 的地址, 所以 <code>ht[0]</code> rehash 之后 key 会指向 null, 最后变成空表.</p></blockquote><p><strong>容量变化:</strong></p>`,36),v=s("ul",null,[s("li",null,[n("扩容到第一个大于等于需要扩容的哈希表的键值对数量的 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mi",null,"n")])]),s("annotation",{encoding:"application/x-tex"},"2^n")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6644em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"n")])])])])])])])])])]),n(".")]),s("li",null,[n("缩容到第一个大于等于需要缩容的哈希表的键值对数量的 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mn",null,"2"),s("mi",null,"n")])]),s("annotation",{encoding:"application/x-tex"},"2^n")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6644em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"n")])])])])])])])])])]),n(".")])],-1),b=e("<p><strong>渐进式 Rehash 原理:</strong></p><ul><li>维持一个索引计数器变量 rehashidx, 初始化为 0, 代表 rehash 开始.</li><li>在每次对字典进行增删改查操作时, 不仅完成应有的操作, 还会将 <code>ht[0]</code> 在 rehashidx 索引上的所有键值对 rehash 到 <code>ht[1]</code> 上, 完成后, <code>rehashidx ++</code>.</li><li>随着字典操作不断进行, 最终 <code>ht[0]</code> 的数据会被完整的 rehash 到 <code>ht[1]</code> 中, 此时将 rehashidx 设置为 -1. 同时删除 <code>ht[0]</code>, 将 <code>ht[1]</code> 作为新的 <code>ht[0]</code>.</li></ul>",2);function g(y,R){const a=c("RouterLink");return o(),p("div",null,[r,s("ul",null,[u,s("li",null,[l(a,{to:"/posts/Middleware/Cache/BloomFilter.html"},{default:i(()=>[n("布隆过滤器")]),_:1}),n(": "),h])]),k,s("p",null,[n("查看 "),l(a,{to:"/posts/Algorithm/DataStructure/SkipList.html"},{default:i(()=>[n("SkipList")]),_:1})]),m,v,b])}const x=t(d,[["render",g],["__file","Cache.html.vue"]]);export{x as default};
